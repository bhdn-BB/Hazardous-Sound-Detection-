{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T19:40:34.247877Z",
     "start_time": "2025-11-15T19:40:34.242737Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:44.527419Z",
     "start_time": "2025-11-15T21:17:44.355262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from audiomentations import Compose\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_df: pd.DataFrame,\n",
    "        filepath_col: str,\n",
    "        target_col: str,\n",
    "        n_classes: int,\n",
    "        sample_rate: int,\n",
    "        target_duration: float,\n",
    "        normalize_audio: bool = True,\n",
    "        mixup_params: Optional[Dict] = None,\n",
    "        is_train: bool = True,\n",
    "        wave_piece: str = \"center\",\n",
    "        audio_transforms: Optional[Compose] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.df = input_df.reset_index(drop=True)\n",
    "\n",
    "        self.filepath_col = filepath_col\n",
    "        self.target_col = target_col\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_duration = target_duration\n",
    "        self.target_sample_count = int(sample_rate * target_duration)\n",
    "\n",
    "        self.normalize_audio = normalize_audio\n",
    "        self.is_train = is_train\n",
    "        self.wave_piece = wave_piece\n",
    "        assert wave_piece in (\"center\", \"random\")\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.audio_transforms = audio_transforms\n",
    "\n",
    "        # Mixup\n",
    "        self.mixup_audio = mixup_params and is_train\n",
    "        self.mixup_params = mixup_params or {\n",
    "            \"prob\": 0.0,\n",
    "            \"alpha\": 0.5,\n",
    "            \"hard_target\": False\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _get_wave(self, idx: int) -> np.ndarray:\n",
    "        filepath = self.df[self.filepath_col].iloc[idx]\n",
    "        wave, _ = librosa.load(filepath, sr=self.sample_rate)\n",
    "        return wave\n",
    "\n",
    "    def _process_wave(self, wave: np.ndarray) -> np.ndarray:\n",
    "        length = len(wave)\n",
    "\n",
    "        if length < self.target_sample_count:\n",
    "            # pad\n",
    "            wave = np.pad(wave, (0, self.target_sample_count - length), mode=\"constant\")\n",
    "        else:\n",
    "            # crop\n",
    "            if self.wave_piece == \"center\":\n",
    "                start = max(0, (length - self.target_sample_count) // 2)\n",
    "            else:\n",
    "                start = np.random.randint(0, length - self.target_sample_count + 1)\n",
    "\n",
    "            wave = wave[start:start + self.target_sample_count]\n",
    "\n",
    "        return wave\n",
    "\n",
    "    def _get_mixup_idx(self):\n",
    "        return np.random.randint(0, len(self.df))\n",
    "\n",
    "    def _prepare_target(self, idx: int, sec_idx: Optional[int] = None):\n",
    "        # MAIN TARGET\n",
    "        cls1 = int(self.df[self.target_col].iloc[idx])  # NUMBER LABEL\n",
    "        y1 = np.zeros(self.n_classes, dtype=np.float32)\n",
    "        y1[cls1] = 1.0\n",
    "\n",
    "        # WITHOUT MIXUP\n",
    "        if sec_idx is None:\n",
    "            return y1\n",
    "\n",
    "        # MIXUP TARGET\n",
    "        cls2 = int(self.df[self.target_col].iloc[sec_idx])\n",
    "        y2 = np.zeros(self.n_classes, dtype=np.float32)\n",
    "        y2[cls2] = 1.0\n",
    "\n",
    "        alpha = self.mixup_params[\"alpha\"]\n",
    "        y_mix = alpha * y1 + (1 - alpha) * y2\n",
    "\n",
    "        if self.mixup_params[\"hard_target\"]:\n",
    "            y_mix = (y_mix > 0).astype(np.float32)\n",
    "\n",
    "        return y_mix\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # MAIN WAVE\n",
    "        wave = self._get_wave(idx)\n",
    "        wave = self._process_wave(wave)\n",
    "\n",
    "        # MIXUP WAVE\n",
    "        if self.mixup_audio and np.random.rand() < self.mixup_params[\"prob\"]:\n",
    "            sec_idx = self._get_mixup_idx()\n",
    "            sec_wave = self._get_wave(sec_idx)\n",
    "            sec_wave = self._process_wave(sec_wave)\n",
    "\n",
    "            alpha = self.mixup_params[\"alpha\"]\n",
    "            wave = alpha * wave + (1 - alpha) * sec_wave\n",
    "            target = self._prepare_target(idx, sec_idx)\n",
    "        else:\n",
    "            target = self._prepare_target(idx)\n",
    "\n",
    "        # AUGMENTATIONS\n",
    "        if self.audio_transforms and self.is_train:\n",
    "            wave = self.audio_transforms(samples=wave, sample_rate=self.sample_rate)\n",
    "\n",
    "        # NORMALIZE\n",
    "        if self.normalize_audio:\n",
    "            wave = librosa.util.normalize(wave)\n",
    "\n",
    "        return torch.from_numpy(wave).float(), torch.from_numpy(target).float()\n"
   ],
   "id": "fdd40374ef90f36c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:45.347139Z",
     "start_time": "2025-11-15T21:17:45.340693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain\n",
    "\n",
    "def get_augmentations():\n",
    "    audio_transforms = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "        Shift(min_shift=-0.1, max_shift=0.1, p=0.5),\n",
    "        Gain(min_gain_db=-6, max_gain_db=6, p=0.5),\n",
    "    ])\n",
    "    return audio_transforms"
   ],
   "id": "3a962c80a9b837a8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:45.973938Z",
     "start_time": "2025-11-15T21:17:45.962796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class AudioForward(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_function,\n",
    "        output_key,\n",
    "        input_key,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_function = loss_function\n",
    "        self.output_key = output_key\n",
    "        self.input_key = input_key\n",
    "\n",
    "    def forward(self, runner, batch, epoch=None):\n",
    "        specs, targets = batch\n",
    "        output = runner.model(specs)\n",
    "        output[\"sigmoid_predictions\"] = torch.sigmoid(output[\"logits\"])\n",
    "        output[\"softmax_predictions\"] = torch.softmax(output[\"logits\"], dim=-1)\n",
    "        inputs = {\n",
    "            \"specs\": specs,\n",
    "            \"targets\": targets,\n",
    "            \"targets_1d\": targets.argmax(dim=-1),\n",
    "        }\n",
    "        losses = {\n",
    "            \"loss\": self.loss_function(\n",
    "                output[self.output_key],\n",
    "                inputs[self.input_key],\n",
    "            )\n",
    "        }\n",
    "        return losses, inputs, output"
   ],
   "id": "624c07d3819241cd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.811357Z",
     "start_time": "2025-11-15T21:17:46.601547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lightning\n",
    "\n",
    "\n",
    "class LitTrainer(lightning.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        forward,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        scheduler_params,\n",
    "        batch_key,\n",
    "        metric_input_key,\n",
    "        metric_output_key,\n",
    "        val_metrics,\n",
    "        train_metrics,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self._forward = forward\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self._scheduler_params = scheduler_params\n",
    "        self._batch_key = batch_key\n",
    "\n",
    "        self._metric_input_key = metric_input_key\n",
    "        self._metric_output_key = metric_output_key\n",
    "        self._val_metrics = val_metrics\n",
    "        self._train_metrics = train_metrics\n",
    "\n",
    "    def _aggregate_outputs(self, losses, inputs, outputs):\n",
    "        united = losses\n",
    "        united.update({\"input_\" + k: v for k, v in inputs.items()})\n",
    "        united.update({\"output_\" + k: v for k, v in outputs.items()})\n",
    "        return united\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        losses, inputs, outputs = self._forward(self, batch, epoch=self.current_epoch)\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            self.log(\n",
    "                \"train/\" + k,\n",
    "                v,\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"train/avg_\" + k,\n",
    "                v,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "        self.log(\n",
    "            \"train/model_time\",\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/avg_model_time\",\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return self._aggregate_outputs(losses, inputs, outputs)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        losses, inputs, outputs = self._forward(self, batch, epoch=self.current_epoch)\n",
    "\n",
    "        if self._val_metrics is not None:\n",
    "            self._val_metrics.update(\n",
    "                outputs[self._metric_output_key],\n",
    "                inputs[self._metric_input_key]\n",
    "            )\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            self.log(\n",
    "                \"valid/\" + k,\n",
    "                v,\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"valid/avg_\" + k,\n",
    "                v,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "        self.log(\n",
    "            \"valid/model_time\",\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"valid/avg_model_time\",\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        return self._aggregate_outputs(losses, inputs, outputs)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metric_values = self._val_metrics.compute()\n",
    "        self.log_dict(\n",
    "            {\"valid/\"+k:v for k,v in metric_values.items()},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=False,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self._val_metrics.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        scheduler = {\"scheduler\": self._scheduler}\n",
    "        scheduler.update(self._scheduler_params)\n",
    "        return (\n",
    "            [self._optimizer], [scheduler],\n",
    "        )"
   ],
   "id": "2a1405b46499035f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.824985Z",
     "start_time": "2025-11-15T21:17:54.820716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "TRAIN_PATH = '/data/train.csv'\n",
    "VALID_PATH = '/data/val.csv'"
   ],
   "id": "53d09fcad51eec72",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.848380Z",
     "start_time": "2025-11-15T21:17:54.837267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VALID_PATH)"
   ],
   "id": "9fd2114d17bd3070",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.868150Z",
     "start_time": "2025-11-15T21:17:54.861953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classes = ['siren', 'gunshot', 'explosion', 'casual']\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "train_dataset = AudioDataset(\n",
    "    input_df=train_df,\n",
    "    filepath_col='path',\n",
    "    target_col='target',\n",
    "    n_classes=4,               # <--- ось тут тільки число!\n",
    "    sample_rate=16000,\n",
    "    target_duration=10,\n",
    "    audio_transforms=get_augmentations()\n",
    ")\n",
    "val_dataset = AudioDataset(\n",
    "    input_df=val_df,\n",
    "    filepath_col='path',\n",
    "    target_col='target',\n",
    "    n_classes=4,\n",
    "    sample_rate=16000,\n",
    "    target_duration=10,\n",
    ")"
   ],
   "id": "e83f0b66c862028e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.886562Z",
     "start_time": "2025-11-15T21:17:54.881961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "7f2adc1fe4ed5318",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.907719Z",
     "start_time": "2025-11-15T21:17:54.900381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class GeMGlobalBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, p: float = 3., eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.pool = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clamp(min=self.eps).pow(self.p)\n",
    "        x = self.pool(x)\n",
    "        x = x.pow(1.0 / self.p)\n",
    "        return x.view(x.size(0), x.size(1))"
   ],
   "id": "e297d28f9fafa889",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:57.268345Z",
     "start_time": "2025-11-15T21:17:54.921377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from typing import Dict, Any, Optional, List\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio.functional import amplitude_to_DB\n",
    "from torchaudio.transforms import FrequencyMasking, TimeMasking\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6, normalize_standart=True, normalize_minmax=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalize_standart = normalize_standart\n",
    "        self.normalize_minmax = normalize_minmax\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.normalize_standart:\n",
    "            mean = X.mean((-2, -1), keepdim=True)\n",
    "            std = X.std((-2, -1), keepdim=True)\n",
    "            X = (X - mean) / (std + self.eps)\n",
    "        if self.normalize_minmax:\n",
    "            norm_max = torch.amax(X, dim=(-2, -1), keepdim=True)\n",
    "            norm_min = torch.amin(X, dim=(-2, -1), keepdim=True)\n",
    "            X = (X - norm_min) / (norm_max - norm_min + self.eps)\n",
    "        return X\n",
    "\n",
    "\n",
    "class CustomMasking(nn.Module):\n",
    "    def __init__(self, mask_max_length: int, mask_max_masks: int, p=1.0, inplace=True):\n",
    "        super().__init__()\n",
    "        assert isinstance(mask_max_masks, int) and mask_max_masks > 0\n",
    "        self.mask_max_masks = mask_max_masks\n",
    "        self.mask_max_length = mask_max_length\n",
    "        self.mask_module = None\n",
    "        self.p = p\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.inplace:\n",
    "            output = x.clone()\n",
    "        for i in range(x.shape[0]):\n",
    "            if np.random.binomial(n=1, p=self.p):\n",
    "                n_applies = np.random.randint(low=1, high=self.mask_max_masks + 1)\n",
    "                for _ in range(n_applies):\n",
    "                    if self.inplace:\n",
    "                        x[i : i + 1] = self.mask_module(x[i : i + 1])\n",
    "                    else:\n",
    "                        output[i : i + 1] = self.mask_module(output[i : i + 1])\n",
    "        if self.inplace:\n",
    "            return x\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "class CustomTimeMasking(CustomMasking):\n",
    "    def __init__(self, mask_max_length: int, mask_max_masks: int, p=1.0, inplace=True):\n",
    "        super().__init__(mask_max_length=mask_max_length, mask_max_masks=mask_max_masks, p=p, inplace=inplace)\n",
    "        self.mask_module = TimeMasking(time_mask_param=mask_max_length)\n",
    "\n",
    "\n",
    "class CustomFreqMasking(CustomMasking):\n",
    "    def __init__(self, mask_max_length: int, mask_max_masks: int, p=1.0, inplace=True):\n",
    "        super().__init__(mask_max_length=mask_max_length, mask_max_masks=mask_max_masks, p=p, inplace=inplace)\n",
    "        self.mask_module = FrequencyMasking(freq_mask_param=mask_max_length)\n",
    "\n",
    "\n",
    "class ChannelAgnosticAmplitudeToDB(nn.Module):\n",
    "    def __init__(self, stype: str = \"power\", top_db: Optional[float] = None):\n",
    "        super().__init__()\n",
    "        self.stype = stype\n",
    "        if top_db is not None and top_db < 0:\n",
    "            raise ValueError(\"top_db must be positive value\")\n",
    "        self.top_db = top_db\n",
    "        self.multiplier = 10.0 if stype == \"power\" else 20.0\n",
    "        self.amin = 1e-10\n",
    "        self.ref_value = 1.0\n",
    "        self.db_multiplier = math.log10(max(self.amin, self.ref_value))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.dim() in [3, 4], f\"Expected 3D or 4D tensor, but got {x.dim()}D tensor\"\n",
    "\n",
    "        add_fake_channel = False\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "            add_fake_channel = True\n",
    "\n",
    "        x_db = amplitude_to_DB(x, self.multiplier, self.amin, self.db_multiplier, self.top_db)\n",
    "\n",
    "        if add_fake_channel:\n",
    "            x_db = x_db.squeeze(1)\n",
    "        return x_db\n",
    "\n",
    "class SpecCNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone: str,\n",
    "            device: str,\n",
    "            n_classes: int,\n",
    "            classifier_dropout: float,\n",
    "            spec_params: Dict[str, Any],\n",
    "            top_db: float,\n",
    "            normalize_config: Dict[str, bool],\n",
    "            pretrained: bool,\n",
    "            pool_type: str,\n",
    "            out_indices: List[int],\n",
    "            in_chans: int,\n",
    "            timm_kwargs: Optional[Dict],\n",
    "            spec_augment_config: Optional[Dict[str, Any]]\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        timm_kwargs = {} if timm_kwargs is None else timm_kwargs\n",
    "        self.out_indices = None if out_indices == \"None\" else tuple(out_indices)\n",
    "        self.n_specs = in_chans\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.spectogram_extractor = nn.Sequential(\n",
    "            MelSpectrogram(**spec_params),\n",
    "            ChannelAgnosticAmplitudeToDB(top_db=top_db),\n",
    "            NormalizeMelSpec(**normalize_config),\n",
    "        )\n",
    "\n",
    "        if spec_augment_config is not None:\n",
    "            self.spec_augment = []\n",
    "            if \"freq_mask\" in spec_augment_config:\n",
    "                self.spec_augment.append(CustomFreqMasking(**spec_augment_config[\"freq_mask\"]))\n",
    "            if \"time_mask\" in spec_augment_config:\n",
    "                self.spec_augment.append(CustomTimeMasking(**spec_augment_config[\"time_mask\"]))\n",
    "            self.spec_augment = nn.Sequential(*self.spec_augment)\n",
    "        else:\n",
    "            self.spec_augment = None\n",
    "\n",
    "        # model\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=self.n_specs,\n",
    "            exportable=True,\n",
    "            out_indices=self.out_indices,\n",
    "            **timm_kwargs,\n",
    "        )\n",
    "\n",
    "        print(self.backbone.feature_info.channels())\n",
    "\n",
    "        feature_dims = self.backbone.feature_info.channels() if self.out_indices is not None else [\n",
    "            self.backbone.feature_info.channels()[-1]]\n",
    "        print(f\"feature dims: {feature_dims}\")\n",
    "\n",
    "        # pooling\n",
    "        pools: List[nn.Module] = []\n",
    "        if pool_type.lower() == \"gem\":\n",
    "            pools = [GeMGlobalBlock() for _ in feature_dims]\n",
    "        elif pool_type.lower() == \"adavg\":\n",
    "            pools = [\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(start_dim=1)\n",
    "                )\n",
    "                for _ in feature_dims\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pool_type={pool_type!r}; choose 'gem' or 'avg'\")\n",
    "\n",
    "        self.pool = nn.ModuleList(pools)\n",
    "\n",
    "        self.emb_dim = sum(feature_dims)\n",
    "\n",
    "        # head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=classifier_dropout),\n",
    "            nn.Linear(self.emb_dim, n_classes),\n",
    "        )\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input, return_spec_feature=False, return_cnn_emb=False):\n",
    "\n",
    "        # specs\n",
    "        specs = self.spectogram_extractor(input)\n",
    "\n",
    "        # multi channel mode support\n",
    "        specs = specs.unsqueeze(1).expand(-1, self.n_specs, -1, -1).contiguous()\n",
    "\n",
    "        if self.spec_augment is not None and self.training:\n",
    "            specs = self.spec_augment(specs)\n",
    "        if return_spec_feature:\n",
    "            return specs\n",
    "\n",
    "        # features - list of stages\n",
    "        features = self.backbone(specs)\n",
    "\n",
    "        if self.out_indices is None:\n",
    "            features = [features[-1]]\n",
    "\n",
    "        pooled = [p(fmap) for fmap, p in zip(features, self.pool)]\n",
    "\n",
    "        emb = torch.cat(pooled, dim=1)\n",
    "\n",
    "        if return_cnn_emb:\n",
    "            return emb\n",
    "\n",
    "        logits = self.classifier(emb)\n",
    "\n",
    "        return {\"logits\": logits}"
   ],
   "id": "5dcd47f8046658b0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:57.287496Z",
     "start_time": "2025-11-15T21:17:57.279679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "PATH2CONFIG = 'D:\\\\audio_cls_coursework\\\\src\\\\model\\\\model_config.yml'\n",
    "with open(PATH2CONFIG, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "device = cfg[\"device\"]\n",
    "n_classes = cfg[\"n_classes\"]\n",
    "spec_params = cfg[\"spec_params\"]\n",
    "normalize_config = cfg[\"normalize_config\"]\n",
    "backbone = cfg[\"backbone\"]\n",
    "pretrained = cfg[\"pretrained\"]\n",
    "pool_type = cfg[\"pool_type\"]\n",
    "out_indices = cfg[\"out_indices\"]\n",
    "in_chans = cfg[\"in_chans\"]\n",
    "timm_kwargs = cfg[\"timm_kwargs\"]\n"
   ],
   "id": "c45da194b161ad88",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SpecCNNClassifier(\n",
    "    backbone=cfg[\"backbone\"],\n",
    "    device=device,\n",
    "    n_classes=cfg[\"n_classes\"],\n",
    "    classifier_dropout=cfg[\"classifier_dropout\"],\n",
    "    spec_params=cfg[\"spec_params\"],\n",
    "    top_db=cfg[\"top_db\"],\n",
    "    normalize_config=cfg[\"normalize_config\"],\n",
    "    pretrained=cfg[\"pretrained\"],\n",
    "    pool_type=cfg[\"pool_type\"],\n",
    "    out_indices=cfg[\"out_indices\"],\n",
    "    in_chans=cfg[\"in_chans\"],\n",
    "    timm_kwargs=cfg[\"timm_kwargs\"],\n",
    "    spec_augment_config=None\n",
    ")\n"
   ],
   "id": "1b9ece46c6d16bbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:21:20.377906Z",
     "start_time": "2025-11-15T21:21:20.358094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchmetrics import MetricCollection\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        reduction: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=inputs,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "# --- Параметри ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_classes = 206\n",
    "precision_mode = \"16-mixed\"\n",
    "train_strategy = \"auto\"\n",
    "n_epochs = 10\n",
    "log_every_n_steps = 4\n",
    "\n",
    "\n",
    "\n",
    "metric_names = [\"rocauc\"]\n",
    "metric_params = {\"average\": \"macro\", \"task\": \"multiclass\", \"num_classes\": n_classes}\n",
    "KEY2LOSSES = {\n",
    "    \"bce\" : torch.nn.BCEWithLogitsLoss,\n",
    "    'ce': torch.nn.CrossEntropyLoss,\n",
    "    'focal': FocalLoss,\n",
    "}\n",
    "loss_fn = KEY2LOSSES['focal']\n",
    "forward = AudioForward(\n",
    "    loss_function=KEY2LOSSES['focal'],\n",
    "    input_key=\"targets\",\n",
    "    output_key=\"logits\",\n",
    ")\n",
    "import torchmetrics\n",
    "\n",
    "KEY2METRICS = {\n",
    "    \"f1\" : torchmetrics.F1Score,\n",
    "    'recall':torchmetrics.Recall,\n",
    "    'precision':torchmetrics.Precision,\n",
    "    'accuracy':torchmetrics.Accuracy,\n",
    "    'rocauc': torchmetrics.AUROC\n",
    "}\n",
    "# --- Метрики ---\n",
    "metrics = MetricCollection([KEY2METRICS[name](**metric_params) for name in metric_names])"
   ],
   "id": "1215b289fb405d8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch import optim\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)*n_epochs, eta_min=1e-6)\n",
    "\n",
    "lightning_model = LitTrainer(\n",
    "    model=model,\n",
    "    forward=forward,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params={\"interval\": \"step\"},\n",
    "    batch_key=\"specs\",\n",
    "    metric_input_key=\"targets_1d\",\n",
    "    metric_output_key=\"sigmoid_predictions\",\n",
    "    val_metrics=metrics,\n",
    "    train_metrics=metrics,\n",
    ")\n",
    "\n",
    "wandb_logger = pl_loggers.WandbLogger(project=\"audio_project\", log_model=True)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor=\"valid/rocauc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=3,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    max_epochs=n_epochs,\n",
    "    precision=precision_mode,\n",
    "    strategy=train_strategy,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_cb, lr_monitor],\n",
    "    log_every_n_steps=log_every_n_steps,\n",
    ")\n",
    "\n",
    "# --- Навчання ---\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ],
   "id": "aee3c24e3c0d30b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2ff68831e216bd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de7cdf7798e40aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae8ca34f12270046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "226e6684f4b9c89e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
