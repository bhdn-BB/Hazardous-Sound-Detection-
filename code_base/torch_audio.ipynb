{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T19:40:34.247877Z",
     "start_time": "2025-11-15T19:40:34.242737Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:44.527419Z",
     "start_time": "2025-11-15T21:17:44.355262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from audiomentations import Compose\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_df: pd.DataFrame,\n",
    "        filepath_col: str,\n",
    "        target_col: str,\n",
    "        n_classes: int,\n",
    "        sample_rate: int,\n",
    "        target_duration: float,\n",
    "        normalize_audio: bool = True,\n",
    "        mixup_params: Optional[Dict] = None,\n",
    "        is_train: bool = True,\n",
    "        wave_piece: str = \"center\",\n",
    "        audio_transforms: Optional[Compose] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.df = input_df.reset_index(drop=True)\n",
    "\n",
    "        self.filepath_col = filepath_col\n",
    "        self.target_col = target_col\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_duration = target_duration\n",
    "        self.target_sample_count = int(sample_rate * target_duration)\n",
    "\n",
    "        self.normalize_audio = normalize_audio\n",
    "        self.is_train = is_train\n",
    "        self.wave_piece = wave_piece\n",
    "        assert wave_piece in (\"center\", \"random\")\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.audio_transforms = audio_transforms\n",
    "\n",
    "        # Mixup\n",
    "        self.mixup_audio = mixup_params and is_train\n",
    "        self.mixup_params = mixup_params or {\n",
    "            \"prob\": 0.0,\n",
    "            \"alpha\": 0.5,\n",
    "            \"hard_target\": False\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _get_wave(self, idx: int) -> np.ndarray:\n",
    "        filepath = self.df[self.filepath_col].iloc[idx]\n",
    "        wave, _ = librosa.load(filepath, sr=self.sample_rate)\n",
    "        return wave\n",
    "\n",
    "    def _process_wave(self, wave: np.ndarray) -> np.ndarray:\n",
    "        length = len(wave)\n",
    "\n",
    "        if length < self.target_sample_count:\n",
    "            # pad\n",
    "            wave = np.pad(wave, (0, self.target_sample_count - length), mode=\"constant\")\n",
    "        else:\n",
    "            # crop\n",
    "            if self.wave_piece == \"center\":\n",
    "                start = max(0, (length - self.target_sample_count) // 2)\n",
    "            else:\n",
    "                start = np.random.randint(0, length - self.target_sample_count + 1)\n",
    "\n",
    "            wave = wave[start:start + self.target_sample_count]\n",
    "\n",
    "        return wave\n",
    "\n",
    "    def _get_mixup_idx(self):\n",
    "        return np.random.randint(0, len(self.df))\n",
    "\n",
    "    def _prepare_target(self, idx: int, sec_idx: Optional[int] = None):\n",
    "        # MAIN TARGET\n",
    "        cls1 = int(self.df[self.target_col].iloc[idx])  # NUMBER LABEL\n",
    "        y1 = np.zeros(self.n_classes, dtype=np.float32)\n",
    "        y1[cls1] = 1.0\n",
    "\n",
    "        # WITHOUT MIXUP\n",
    "        if sec_idx is None:\n",
    "            return y1\n",
    "\n",
    "        # MIXUP TARGET\n",
    "        cls2 = int(self.df[self.target_col].iloc[sec_idx])\n",
    "        y2 = np.zeros(self.n_classes, dtype=np.float32)\n",
    "        y2[cls2] = 1.0\n",
    "\n",
    "        alpha = self.mixup_params[\"alpha\"]\n",
    "        y_mix = alpha * y1 + (1 - alpha) * y2\n",
    "\n",
    "        if self.mixup_params[\"hard_target\"]:\n",
    "            y_mix = (y_mix > 0).astype(np.float32)\n",
    "\n",
    "        return y_mix\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # MAIN WAVE\n",
    "        wave = self._get_wave(idx)\n",
    "        wave = self._process_wave(wave)\n",
    "\n",
    "        # MIXUP WAVE\n",
    "        if self.mixup_audio and np.random.rand() < self.mixup_params[\"prob\"]:\n",
    "            sec_idx = self._get_mixup_idx()\n",
    "            sec_wave = self._get_wave(sec_idx)\n",
    "            sec_wave = self._process_wave(sec_wave)\n",
    "\n",
    "            alpha = self.mixup_params[\"alpha\"]\n",
    "            wave = alpha * wave + (1 - alpha) * sec_wave\n",
    "            target = self._prepare_target(idx, sec_idx)\n",
    "        else:\n",
    "            target = self._prepare_target(idx)\n",
    "\n",
    "        # AUGMENTATIONS\n",
    "        if self.audio_transforms and self.is_train:\n",
    "            wave = self.audio_transforms(samples=wave, sample_rate=self.sample_rate)\n",
    "\n",
    "        # NORMALIZE\n",
    "        if self.normalize_audio:\n",
    "            wave = librosa.util.normalize(wave)\n",
    "\n",
    "        return torch.from_numpy(wave).float(), torch.from_numpy(target).float()\n"
   ],
   "id": "fdd40374ef90f36c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:45.347139Z",
     "start_time": "2025-11-15T21:17:45.340693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain\n",
    "\n",
    "def get_augmentations():\n",
    "    audio_transforms = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "        Shift(min_shift=-0.1, max_shift=0.1, p=0.5),\n",
    "        Gain(min_gain_db=-6, max_gain_db=6, p=0.5),\n",
    "    ])\n",
    "    return audio_transforms"
   ],
   "id": "3a962c80a9b837a8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:45.973938Z",
     "start_time": "2025-11-15T21:17:45.962796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class AudioForward(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_function,\n",
    "        output_key,\n",
    "        input_key,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_function = loss_function\n",
    "        self.output_key = output_key\n",
    "        self.input_key = input_key\n",
    "\n",
    "    def forward(self, runner, batch, epoch=None):\n",
    "        specs, targets = batch\n",
    "        output = runner.model(specs)\n",
    "        output[\"sigmoid_predictions\"] = torch.sigmoid(output[\"logits\"])\n",
    "        output[\"softmax_predictions\"] = torch.softmax(output[\"logits\"], dim=-1)\n",
    "        inputs = {\n",
    "            \"specs\": specs,\n",
    "            \"targets\": targets,\n",
    "            \"targets_1d\": targets.argmax(dim=-1),\n",
    "        }\n",
    "        losses = {\n",
    "            \"loss\": self.loss_function(\n",
    "                output[self.output_key],\n",
    "                inputs[self.input_key],\n",
    "            )\n",
    "        }\n",
    "        return losses, inputs, output"
   ],
   "id": "624c07d3819241cd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.811357Z",
     "start_time": "2025-11-15T21:17:46.601547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lightning\n",
    "\n",
    "\n",
    "class LitTrainer(lightning.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        forward,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        scheduler_params,\n",
    "        batch_key,\n",
    "        metric_input_key,\n",
    "        metric_output_key,\n",
    "        val_metrics,\n",
    "        train_metrics,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self._forward = forward\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self._scheduler_params = scheduler_params\n",
    "        self._batch_key = batch_key\n",
    "\n",
    "        self._metric_input_key = metric_input_key\n",
    "        self._metric_output_key = metric_output_key\n",
    "        self._val_metrics = val_metrics\n",
    "        self._train_metrics = train_metrics\n",
    "\n",
    "    def _aggregate_outputs(self, losses, inputs, outputs):\n",
    "        united = losses\n",
    "        united.update({\"input_\" + k: v for k, v in inputs.items()})\n",
    "        united.update({\"output_\" + k: v for k, v in outputs.items()})\n",
    "        return united\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        losses, inputs, outputs = self._forward(self, batch, epoch=self.current_epoch)\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            self.log(\n",
    "                \"train/\" + k,\n",
    "                v,\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"train/avg_\" + k,\n",
    "                v,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "        self.log(\n",
    "            \"train/model_time\",\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/avg_model_time\",\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return self._aggregate_outputs(losses, inputs, outputs)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        losses, inputs, outputs = self._forward(self, batch, epoch=self.current_epoch)\n",
    "\n",
    "        if self._val_metrics is not None:\n",
    "            self._val_metrics.update(\n",
    "                outputs[self._metric_output_key],\n",
    "                inputs[self._metric_input_key]\n",
    "            )\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            self.log(\n",
    "                \"valid/\" + k,\n",
    "                v,\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"valid/avg_\" + k,\n",
    "                v,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                batch_size=inputs[self._batch_key].shape[0],\n",
    "                sync_dist=True,\n",
    "            )\n",
    "        self.log(\n",
    "            \"valid/model_time\",\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"valid/avg_model_time\",\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            batch_size=1,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        return self._aggregate_outputs(losses, inputs, outputs)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metric_values = self._val_metrics.compute()\n",
    "        self.log_dict(\n",
    "            {\"valid/\"+k:v for k,v in metric_values.items()},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=False,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self._val_metrics.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        scheduler = {\"scheduler\": self._scheduler}\n",
    "        scheduler.update(self._scheduler_params)\n",
    "        return (\n",
    "            [self._optimizer], [scheduler],\n",
    "        )"
   ],
   "id": "2a1405b46499035f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.824985Z",
     "start_time": "2025-11-15T21:17:54.820716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "TRAIN_PATH = '/data/train.csv'\n",
    "VALID_PATH = '/data/val.csv'"
   ],
   "id": "53d09fcad51eec72",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.848380Z",
     "start_time": "2025-11-15T21:17:54.837267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VALID_PATH)"
   ],
   "id": "9fd2114d17bd3070",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.868150Z",
     "start_time": "2025-11-15T21:17:54.861953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classes = ['siren', 'gunshot', 'explosion', 'casual']\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "train_dataset = AudioDataset(\n",
    "    input_df=train_df,\n",
    "    filepath_col='path',\n",
    "    target_col='target',\n",
    "    n_classes=4,               # <--- ось тут тільки число!\n",
    "    sample_rate=16000,\n",
    "    target_duration=10,\n",
    "    audio_transforms=get_augmentations()\n",
    ")\n",
    "val_dataset = AudioDataset(\n",
    "    input_df=val_df,\n",
    "    filepath_col='path',\n",
    "    target_col='target',\n",
    "    n_classes=4,\n",
    "    sample_rate=16000,\n",
    "    target_duration=10,\n",
    ")"
   ],
   "id": "e83f0b66c862028e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.886562Z",
     "start_time": "2025-11-15T21:17:54.881961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "7f2adc1fe4ed5318",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:54.907719Z",
     "start_time": "2025-11-15T21:17:54.900381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class GeMGlobalBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, p: float = 3., eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.pool = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clamp(min=self.eps).pow(self.p)\n",
    "        x = self.pool(x)\n",
    "        x = x.pow(1.0 / self.p)\n",
    "        return x.view(x.size(0), x.size(1))"
   ],
   "id": "e297d28f9fafa889",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:57.268345Z",
     "start_time": "2025-11-15T21:17:54.921377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from typing import Dict, Any, Optional, List\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio.functional import amplitude_to_DB\n",
    "from torchaudio.transforms import FrequencyMasking, TimeMasking\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6, normalize_standart=True, normalize_minmax=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalize_standart = normalize_standart\n",
    "        self.normalize_minmax = normalize_minmax\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.normalize_standart:\n",
    "            mean = X.mean((-2, -1), keepdim=True)\n",
    "            std = X.std((-2, -1), keepdim=True)\n",
    "            X = (X - mean) / (std + self.eps)\n",
    "        if self.normalize_minmax:\n",
    "            norm_max = torch.amax(X, dim=(-2, -1), keepdim=True)\n",
    "            norm_min = torch.amin(X, dim=(-2, -1), keepdim=True)\n",
    "            X = (X - norm_min) / (norm_max - norm_min + self.eps)\n",
    "        return X\n",
    "\n",
    "\n",
    "class CustomMasking(nn.Module):\n",
    "    def __init__(self, mask_max_length: int, mask_max_masks: int, p=1.0, inplace=True):\n",
    "        super().__init__()\n",
    "        assert isinstance(mask_max_masks, int) and mask_max_masks > 0\n",
    "        self.mask_max_masks = mask_max_masks\n",
    "        self.mask_max_length = mask_max_length\n",
    "        self.mask_module = None\n",
    "        self.p = p\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.inplace:\n",
    "            output = x.clone()\n",
    "        for i in range(x.shape[0]):\n",
    "            if np.random.binomial(n=1, p=self.p):\n",
    "                n_applies = np.random.randint(low=1, high=self.mask_max_masks + 1)\n",
    "                for _ in range(n_applies):\n",
    "                    if self.inplace:\n",
    "                        x[i : i + 1] = self.mask_module(x[i : i + 1])\n",
    "                    else:\n",
    "                        output[i : i + 1] = self.mask_module(output[i : i + 1])\n",
    "        if self.inplace:\n",
    "            return x\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "class CustomTimeMasking(CustomMasking):\n",
    "    def __init__(self, mask_max_length: int, mask_max_masks: int, p=1.0, inplace=True):\n",
    "        super().__init__(mask_max_length=mask_max_length, mask_max_masks=mask_max_masks, p=p, inplace=inplace)\n",
    "        self.mask_module = TimeMasking(time_mask_param=mask_max_length)\n",
    "\n",
    "\n",
    "class CustomFreqMasking(CustomMasking):\n",
    "    def __init__(self, mask_max_length: int, mask_max_masks: int, p=1.0, inplace=True):\n",
    "        super().__init__(mask_max_length=mask_max_length, mask_max_masks=mask_max_masks, p=p, inplace=inplace)\n",
    "        self.mask_module = FrequencyMasking(freq_mask_param=mask_max_length)\n",
    "\n",
    "\n",
    "class ChannelAgnosticAmplitudeToDB(nn.Module):\n",
    "    def __init__(self, stype: str = \"power\", top_db: Optional[float] = None):\n",
    "        super().__init__()\n",
    "        self.stype = stype\n",
    "        if top_db is not None and top_db < 0:\n",
    "            raise ValueError(\"top_db must be positive value\")\n",
    "        self.top_db = top_db\n",
    "        self.multiplier = 10.0 if stype == \"power\" else 20.0\n",
    "        self.amin = 1e-10\n",
    "        self.ref_value = 1.0\n",
    "        self.db_multiplier = math.log10(max(self.amin, self.ref_value))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.dim() in [3, 4], f\"Expected 3D or 4D tensor, but got {x.dim()}D tensor\"\n",
    "\n",
    "        add_fake_channel = False\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "            add_fake_channel = True\n",
    "\n",
    "        x_db = amplitude_to_DB(x, self.multiplier, self.amin, self.db_multiplier, self.top_db)\n",
    "\n",
    "        if add_fake_channel:\n",
    "            x_db = x_db.squeeze(1)\n",
    "        return x_db\n",
    "\n",
    "class SpecCNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone: str,\n",
    "            device: str,\n",
    "            n_classes: int,\n",
    "            classifier_dropout: float,\n",
    "            spec_params: Dict[str, Any],\n",
    "            top_db: float,\n",
    "            normalize_config: Dict[str, bool],\n",
    "            pretrained: bool,\n",
    "            pool_type: str,\n",
    "            out_indices: List[int],\n",
    "            in_chans: int,\n",
    "            timm_kwargs: Optional[Dict],\n",
    "            spec_augment_config: Optional[Dict[str, Any]]\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        timm_kwargs = {} if timm_kwargs is None else timm_kwargs\n",
    "        self.out_indices = None if out_indices == \"None\" else tuple(out_indices)\n",
    "        self.n_specs = in_chans\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.spectogram_extractor = nn.Sequential(\n",
    "            MelSpectrogram(**spec_params),\n",
    "            ChannelAgnosticAmplitudeToDB(top_db=top_db),\n",
    "            NormalizeMelSpec(**normalize_config),\n",
    "        )\n",
    "\n",
    "        if spec_augment_config is not None:\n",
    "            self.spec_augment = []\n",
    "            if \"freq_mask\" in spec_augment_config:\n",
    "                self.spec_augment.append(CustomFreqMasking(**spec_augment_config[\"freq_mask\"]))\n",
    "            if \"time_mask\" in spec_augment_config:\n",
    "                self.spec_augment.append(CustomTimeMasking(**spec_augment_config[\"time_mask\"]))\n",
    "            self.spec_augment = nn.Sequential(*self.spec_augment)\n",
    "        else:\n",
    "            self.spec_augment = None\n",
    "\n",
    "        # model\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=self.n_specs,\n",
    "            exportable=True,\n",
    "            out_indices=self.out_indices,\n",
    "            **timm_kwargs,\n",
    "        )\n",
    "\n",
    "        print(self.backbone.feature_info.channels())\n",
    "\n",
    "        feature_dims = self.backbone.feature_info.channels() if self.out_indices is not None else [\n",
    "            self.backbone.feature_info.channels()[-1]]\n",
    "        print(f\"feature dims: {feature_dims}\")\n",
    "\n",
    "        # pooling\n",
    "        pools: List[nn.Module] = []\n",
    "        if pool_type.lower() == \"gem\":\n",
    "            pools = [GeMGlobalBlock() for _ in feature_dims]\n",
    "        elif pool_type.lower() == \"adavg\":\n",
    "            pools = [\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(start_dim=1)\n",
    "                )\n",
    "                for _ in feature_dims\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pool_type={pool_type!r}; choose 'gem' or 'avg'\")\n",
    "\n",
    "        self.pool = nn.ModuleList(pools)\n",
    "\n",
    "        self.emb_dim = sum(feature_dims)\n",
    "\n",
    "        # head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=classifier_dropout),\n",
    "            nn.Linear(self.emb_dim, n_classes),\n",
    "        )\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input, return_spec_feature=False, return_cnn_emb=False):\n",
    "\n",
    "        # specs\n",
    "        specs = self.spectogram_extractor(input)\n",
    "\n",
    "        # multi channel mode support\n",
    "        specs = specs.unsqueeze(1).expand(-1, self.n_specs, -1, -1).contiguous()\n",
    "\n",
    "        if self.spec_augment is not None and self.training:\n",
    "            specs = self.spec_augment(specs)\n",
    "        if return_spec_feature:\n",
    "            return specs\n",
    "\n",
    "        # features - list of stages\n",
    "        features = self.backbone(specs)\n",
    "\n",
    "        if self.out_indices is None:\n",
    "            features = [features[-1]]\n",
    "\n",
    "        pooled = [p(fmap) for fmap, p in zip(features, self.pool)]\n",
    "\n",
    "        emb = torch.cat(pooled, dim=1)\n",
    "\n",
    "        if return_cnn_emb:\n",
    "            return emb\n",
    "\n",
    "        logits = self.classifier(emb)\n",
    "\n",
    "        return {\"logits\": logits}"
   ],
   "id": "5dcd47f8046658b0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:57.287496Z",
     "start_time": "2025-11-15T21:17:57.279679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "PATH2CONFIG = 'D:\\\\audio_cls_coursework\\\\src\\\\model\\\\model_config.yml'\n",
    "with open(PATH2CONFIG, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# приклад доступу до параметрів\n",
    "device = cfg[\"device\"]\n",
    "n_classes = cfg[\"n_classes\"]\n",
    "spec_params = cfg[\"spec_params\"]\n",
    "normalize_config = cfg[\"normalize_config\"]\n",
    "backbone = cfg[\"backbone\"]\n",
    "pretrained = cfg[\"pretrained\"]\n",
    "pool_type = cfg[\"pool_type\"]\n",
    "out_indices = cfg[\"out_indices\"]\n",
    "in_chans = cfg[\"in_chans\"]\n",
    "timm_kwargs = cfg[\"timm_kwargs\"]\n"
   ],
   "id": "c45da194b161ad88",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:17:57.465038Z",
     "start_time": "2025-11-15T21:17:57.301663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SpecCNNClassifier(\n",
    "    backbone=cfg[\"backbone\"],\n",
    "    device=device,\n",
    "    n_classes=cfg[\"n_classes\"],\n",
    "    classifier_dropout=cfg[\"classifier_dropout\"],\n",
    "    spec_params=cfg[\"spec_params\"],\n",
    "    top_db=cfg[\"top_db\"],\n",
    "    normalize_config=cfg[\"normalize_config\"],\n",
    "    pretrained=cfg[\"pretrained\"],\n",
    "    pool_type=cfg[\"pool_type\"],\n",
    "    out_indices=cfg[\"out_indices\"],\n",
    "    in_chans=cfg[\"in_chans\"],\n",
    "    timm_kwargs=cfg[\"timm_kwargs\"],\n",
    "    spec_augment_config=None\n",
    ")\n"
   ],
   "id": "1b9ece46c6d16bbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 112]\n",
      "feature dims: [40, 112]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:21:20.377906Z",
     "start_time": "2025-11-15T21:21:20.358094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchmetrics import MetricCollection\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        reduction: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=inputs,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "# --- Параметри ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_classes = 206\n",
    "precision_mode = \"16-mixed\"\n",
    "train_strategy = \"auto\"\n",
    "n_epochs = 10\n",
    "log_every_n_steps = 4\n",
    "\n",
    "\n",
    "\n",
    "metric_names = [\"rocauc\"]\n",
    "metric_params = {\"average\": \"macro\", \"task\": \"multiclass\", \"num_classes\": n_classes}\n",
    "KEY2LOSSES = {\n",
    "    \"bce\" : torch.nn.BCEWithLogitsLoss,\n",
    "    'ce': torch.nn.CrossEntropyLoss,\n",
    "    'focal': FocalLoss,\n",
    "}\n",
    "loss_fn = KEY2LOSSES['focal']\n",
    "forward = AudioForward(\n",
    "    loss_function=KEY2LOSSES['focal'],\n",
    "    input_key=\"targets\",\n",
    "    output_key=\"logits\",\n",
    ")\n",
    "import torchmetrics\n",
    "\n",
    "KEY2METRICS = {\n",
    "    \"f1\" : torchmetrics.F1Score,\n",
    "    'recall':torchmetrics.Recall,\n",
    "    'precision':torchmetrics.Precision,\n",
    "    'accuracy':torchmetrics.Accuracy,\n",
    "    'rocauc': torchmetrics.AUROC\n",
    "}\n",
    "# --- Метрики ---\n",
    "metrics = MetricCollection([KEY2METRICS[name](**metric_params) for name in metric_names])"
   ],
   "id": "1215b289fb405d8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:22:28.177430Z",
     "start_time": "2025-11-15T21:21:55.392619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch import optim\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "# --- Оптимізатор + Scheduler ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)*n_epochs, eta_min=1e-6)\n",
    "\n",
    "# --- Lightning Trainer ---\n",
    "lightning_model = LitTrainer(\n",
    "    model=model,\n",
    "    forward=forward,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params={\"interval\": \"step\"},\n",
    "    batch_key=\"specs\",\n",
    "    metric_input_key=\"targets_1d\",\n",
    "    metric_output_key=\"sigmoid_predictions\",\n",
    "    val_metrics=metrics,\n",
    "    train_metrics=metrics,\n",
    ")\n",
    "\n",
    "wandb_logger = pl_loggers.WandbLogger(project=\"audio_project\", log_model=True)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor=\"valid/rocauc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=3,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    max_epochs=n_epochs,\n",
    "    precision=precision_mode,\n",
    "    strategy=train_strategy,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_cb, lr_monitor],\n",
    "    log_every_n_steps=log_every_n_steps,\n",
    ")\n",
    "\n",
    "# --- Навчання ---\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ],
   "id": "aee3c24e3c0d30b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py:508: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "wandb: Currently logged in as: boklahbohdan (detect_kaggle) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20251115_232158-1gpreoao</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/detect_kaggle/audio_project/runs/1gpreoao' target=\"_blank\">solar-grass-1</a></strong> to <a href='https://wandb.ai/detect_kaggle/audio_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/detect_kaggle/audio_project' target=\"_blank\">https://wandb.ai/detect_kaggle/audio_project</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/detect_kaggle/audio_project/runs/1gpreoao' target=\"_blank\">https://wandb.ai/detect_kaggle/audio_project/runs/1gpreoao</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name         | Type              | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model        | SpecCNNClassifier | 3.6 M  | train\n",
      "1 | _forward     | AudioForward      | 0      | train\n",
      "2 | _val_metrics | MetricCollection  | 0      | train\n",
      "-----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.508    Total estimated model params size (MB)\n",
      "333       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28f02955b0414fcca1046e385f334e87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FocalLoss.__init__() missing 1 required positional argument: 'reduction'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 44\u001B[39m\n\u001B[32m     32\u001B[39m trainer = lightning.Trainer(\n\u001B[32m     33\u001B[39m     accelerator=\u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     34\u001B[39m     devices=\u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     40\u001B[39m     log_every_n_steps=log_every_n_steps,\n\u001B[32m     41\u001B[39m )\n\u001B[32m     43\u001B[39m \u001B[38;5;66;03m# --- Навчання ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlightning_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:560\u001B[39m, in \u001B[36mTrainer.fit\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[39m\n\u001B[32m    558\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    559\u001B[39m \u001B[38;5;28mself\u001B[39m.should_stop = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m560\u001B[39m \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:49\u001B[39m, in \u001B[36m_call_and_handle_interrupt\u001B[39m\u001B[34m(trainer, trainer_fn, *args, **kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m trainer.strategy.launcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     48\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[32m     52\u001B[39m     _call_teardown_hook(trainer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:598\u001B[39m, in \u001B[36mTrainer._fit_impl\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[39m\n\u001B[32m    591\u001B[39m     download_model_from_registry(ckpt_path, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m    592\u001B[39m ckpt_path = \u001B[38;5;28mself\u001B[39m._checkpoint_connector._select_ckpt_path(\n\u001B[32m    593\u001B[39m     \u001B[38;5;28mself\u001B[39m.state.fn,\n\u001B[32m    594\u001B[39m     ckpt_path,\n\u001B[32m    595\u001B[39m     model_provided=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    596\u001B[39m     model_connected=\u001B[38;5;28mself\u001B[39m.lightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    597\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m598\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    600\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.stopped\n\u001B[32m    601\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1011\u001B[39m, in \u001B[36mTrainer._run\u001B[39m\u001B[34m(self, model, ckpt_path)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28mself\u001B[39m._signal_connector.register_signal_handlers()\n\u001B[32m   1008\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1009\u001B[39m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[32m   1010\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1011\u001B[39m results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1014\u001B[39m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[32m   1015\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1016\u001B[39m log.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: trainer tearing down\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1053\u001B[39m, in \u001B[36mTrainer._run_stage\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.training:\n\u001B[32m   1052\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[32m-> \u001B[39m\u001B[32m1053\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1054\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.autograd.set_detect_anomaly(\u001B[38;5;28mself\u001B[39m._detect_anomaly):\n\u001B[32m   1055\u001B[39m         \u001B[38;5;28mself\u001B[39m.fit_loop.run()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1082\u001B[39m, in \u001B[36mTrainer._run_sanity_check\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1079\u001B[39m call._call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mon_sanity_check_start\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1081\u001B[39m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1082\u001B[39m \u001B[43mval_loop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1084\u001B[39m call._call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mon_sanity_check_end\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1086\u001B[39m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:179\u001B[39m, in \u001B[36m_no_grad_context.<locals>._decorator\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    177\u001B[39m     context_manager = torch.no_grad\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:145\u001B[39m, in \u001B[36m_EvaluationLoop.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    143\u001B[39m     \u001B[38;5;28mself\u001B[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001B[32m    144\u001B[39m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    147\u001B[39m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[32m    148\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:437\u001B[39m, in \u001B[36m_EvaluationLoop._evaluation_step\u001B[39m\u001B[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[39m\n\u001B[32m    431\u001B[39m hook_name = \u001B[33m\"\u001B[39m\u001B[33mtest_step\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer.testing \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mvalidation_step\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    432\u001B[39m step_args = (\n\u001B[32m    433\u001B[39m     \u001B[38;5;28mself\u001B[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[32m    434\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[32m    435\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[32m    436\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m437\u001B[39m output = \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    439\u001B[39m \u001B[38;5;28mself\u001B[39m.batch_progress.increment_processed()\n\u001B[32m    441\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[32m    442\u001B[39m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:329\u001B[39m, in \u001B[36m_call_strategy_hook\u001B[39m\u001B[34m(trainer, hook_name, *args, **kwargs)\u001B[39m\n\u001B[32m    326\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    328\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m trainer.profiler.profile(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer.strategy.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m329\u001B[39m     output = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    331\u001B[39m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[32m    332\u001B[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:412\u001B[39m, in \u001B[36mStrategy.validation_step\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model != \u001B[38;5;28mself\u001B[39m.lightning_module:\n\u001B[32m    411\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_redirection(\u001B[38;5;28mself\u001B[39m.model, \u001B[38;5;28mself\u001B[39m.lightning_module, \u001B[33m\"\u001B[39m\u001B[33mvalidation_step\u001B[39m\u001B[33m\"\u001B[39m, *args, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m412\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlightning_module\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 83\u001B[39m, in \u001B[36mLitTrainer.validation_step\u001B[39m\u001B[34m(self, batch, batch_idx)\u001B[39m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[32m---> \u001B[39m\u001B[32m83\u001B[39m     losses, inputs, outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     85\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._val_metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     86\u001B[39m         \u001B[38;5;28mself\u001B[39m._val_metrics.update(\n\u001B[32m     87\u001B[39m             outputs[\u001B[38;5;28mself\u001B[39m._metric_output_key],\n\u001B[32m     88\u001B[39m             inputs[\u001B[38;5;28mself\u001B[39m._metric_input_key]\n\u001B[32m     89\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mAudioForward.forward\u001B[39m\u001B[34m(self, runner, batch, epoch)\u001B[39m\n\u001B[32m     20\u001B[39m output[\u001B[33m\"\u001B[39m\u001B[33msoftmax_predictions\u001B[39m\u001B[33m\"\u001B[39m] = torch.softmax(output[\u001B[33m\"\u001B[39m\u001B[33mlogits\u001B[39m\u001B[33m\"\u001B[39m], dim=-\u001B[32m1\u001B[39m)\n\u001B[32m     21\u001B[39m inputs = {\n\u001B[32m     22\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mspecs\u001B[39m\u001B[33m\"\u001B[39m: specs,\n\u001B[32m     23\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtargets\u001B[39m\u001B[33m\"\u001B[39m: targets,\n\u001B[32m     24\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtargets_1d\u001B[39m\u001B[33m\"\u001B[39m: targets.argmax(dim=-\u001B[32m1\u001B[39m),\n\u001B[32m     25\u001B[39m }\n\u001B[32m     26\u001B[39m losses = {\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moutput_key\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minput_key\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m }\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m losses, inputs, output\n",
      "\u001B[31mTypeError\u001B[39m: FocalLoss.__init__() missing 1 required positional argument: 'reduction'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2ff68831e216bd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de7cdf7798e40aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae8ca34f12270046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "226e6684f4b9c89e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
